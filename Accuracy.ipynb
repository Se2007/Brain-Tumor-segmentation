{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import time\n",
    "\n",
    "from torchmetrics import Dice\n",
    "from torchmetrics.aggregation import MeanMetric\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from Benchmark import dataset\n",
    "from methods import unet, unetplusplus\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "from colorama import Fore, Style, init\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "load_path = './saved_model/pretrain-unet-efficientnet.pth'\n",
    "\n",
    "\n",
    "def segment(image, model):\n",
    "  with torch.inference_mode():\n",
    "    prediction = model(image)\n",
    "    return torch.sigmoid(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load test data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_size = 64\n",
    "test_loader = dataset.BraTS20(root='./Benchmark', mode='test', mini=False, memory=False)(batch_size=test_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = unet.UNet(n_channels=3, n_classes=3, bilinear=False).to(device)\n",
    "model = unet.pre_train_unet(in_channels=4, classes=4, encoder_name='efficientnet-b1').to(device)\n",
    "# model = unetplusplus.UnetPlusPlus(encoder_name='efficientnet-b3').to(device)\n",
    "# model = unetplusplus.UnetPlusPlus(encoder_name='resnet18').to(device)\n",
    "\n",
    "\n",
    "sate = torch.load(load_path)\n",
    "model.load_state_dict(sate['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, device='cpu'):\n",
    "    model.eval().to(device)\n",
    "    \n",
    "    # Initialize metrics\n",
    "    iou_score, f1_score, f2_score, accuracy, recall = MeanMetric(), MeanMetric(), MeanMetric(), MeanMetric(), MeanMetric()\n",
    "    dice_metric = Dice(average='micro').to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        # Wrap the test_loader with tqdm for a progress bar\n",
    "        for inputs, targets in tqdm(test_loader, desc=\"Evaluating\", leave=False):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Model predictions\n",
    "            outputs = segment(inputs, model)\n",
    "    \n",
    "            # Convert targets to integer type\n",
    "            targets = targets.to(torch.int32)\n",
    "\n",
    "            # Calculate true positives, false positives, false negatives, true negatives\n",
    "            tp, fp, fn, tn = smp.metrics.get_stats(outputs.cpu(), targets.cpu(), mode='multilabel', threshold=0.5)\n",
    "      \n",
    "            # Update the metrics\n",
    "            iou_score.update(smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\"), weight=len(targets))\n",
    "            f1_score.update(smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"micro\"), weight=len(targets))\n",
    "            f2_score.update(smp.metrics.fbeta_score(tp, fp, fn, tn, beta=2, reduction=\"micro\"), weight=len(targets))\n",
    "            accuracy.update(smp.metrics.accuracy(tp, fp, fn, tn, reduction=\"macro\"), weight=len(targets))\n",
    "            recall.update(smp.metrics.recall(tp, fp, fn, tn, reduction=\"micro-imagewise\"), weight=len(targets))\n",
    "\n",
    "            # Update the Dice metric\n",
    "            dice_metric(outputs, targets)\n",
    "\n",
    "    # Compute final metric values\n",
    "    return (iou_score.compute().item(), \n",
    "            f1_score.compute().item(), \n",
    "            f2_score.compute().item(), \n",
    "            accuracy.compute().item(), \n",
    "            recall.compute().item(), \n",
    "            dice_metric.compute().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "| Metric    |  Value |\n",
      "+-----------+--------+\n",
      "| IoU Score | 87.28% |\n",
      "| F1 Score  | 93.21% |\n",
      "| F2 Score  | 89.73% |\n",
      "| Recall    | 87.55% |\n",
      "| Dice      | 93.21% |\n",
      "+-----------+--------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "iou_score, f1_score, f2_score, accuracy, recall, dice = evaluate(model, test_loader, device='cuda')\n",
    "\n",
    "table = PrettyTable()\n",
    "\n",
    "# Define column names and alignment\n",
    "table.field_names = [\"Metric\", \"Value\"]\n",
    "table.align[\"Metric\"] = \"l\"\n",
    "table.align[\"Value\"] = \"r\"\n",
    "\n",
    "table.add_row([\"IoU Score\", f\"{iou_score:.2%}\"])\n",
    "table.add_row([\"F1 Score\", f\"{f1_score:.2%}\"])\n",
    "table.add_row([\"F2 Score\", f\"{f2_score:.2%}\"])\n",
    "# table.add_row([\"Accuracy\", f\"{accuracy:.2%}\"])\n",
    "table.add_row([\"Recall\", f\"{recall:.2%}\"])\n",
    "table.add_row([\"Dice\", f\"{dice:.2%}\"])\n",
    "\n",
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
